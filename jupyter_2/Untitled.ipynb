{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training..\n",
      "Loading /home/ecbm4040/data...\n",
      "iter:  0  finished, D loss:  1.1485785 , G loss:  10.340448\n",
      "iter:  1  finished, D loss:  27.679518 , G loss:  46.923965\n",
      "iter:  2  finished, D loss:  0.16239472 , G loss:  57.6192\n",
      "iter:  3  finished, D loss:  0.87554413 , G loss:  30.225334\n",
      "iter:  4  finished, D loss:  1.6936502 , G loss:  16.821728\n",
      "iter:  5  finished, D loss:  11.247913 , G loss:  61.848927\n",
      "iter:  6  finished, D loss:  1.7716634 , G loss:  85.54904\n",
      "iter:  7  finished, D loss:  1.3942811 , G loss:  78.24292\n",
      "iter:  8  finished, D loss:  1.2622665 , G loss:  45.596855\n",
      "iter:  9  finished, D loss:  0.46210888 , G loss:  0.17441368\n",
      "iter:  10  finished, D loss:  31.016739 , G loss:  33.082535\n",
      "iter:  11  finished, D loss:  0.7548545 , G loss:  58.541763\n",
      "iter:  12  finished, D loss:  0.81445163 , G loss:  69.59363\n",
      "iter:  13  finished, D loss:  0.2565141 , G loss:  61.211475\n",
      "iter:  14  finished, D loss:  0.53716606 , G loss:  28.91603\n",
      "iter:  15  finished, D loss:  7.845622 , G loss:  51.351593\n",
      "iter:  16  finished, D loss:  0.7251783 , G loss:  58.120804\n",
      "iter:  17  finished, D loss:  0.24721497 , G loss:  48.507164\n",
      "iter:  18  finished, D loss:  1.001975 , G loss:  27.669102\n",
      "iter:  19  finished, D loss:  4.452379 , G loss:  63.314766\n",
      "iter:  20  finished, D loss:  2.9039562 , G loss:  83.06279\n",
      "iter:  21  finished, D loss:  3.8877864 , G loss:  84.83496\n",
      "iter:  22  finished, D loss:  3.8228874 , G loss:  75.12587\n",
      "iter:  23  finished, D loss:  1.8317528 , G loss:  56.12668\n",
      "iter:  24  finished, D loss:  0.3428513 , G loss:  28.907572\n",
      "iter:  25  finished, D loss:  3.6968231 , G loss:  14.563724\n",
      "iter:  26  finished, D loss:  13.764387 , G loss:  66.49698\n",
      "iter:  27  finished, D loss:  1.0158645 , G loss:  102.23038\n",
      "iter:  28  finished, D loss:  0.00015643674 , G loss:  118.61764\n",
      "iter:  29  finished, D loss:  3.4674928 , G loss:  114.76071\n",
      "iter:  30  finished, D loss:  1.5500112 , G loss:  95.20662\n",
      "iter:  31  finished, D loss:  2.7859843 , G loss:  63.83938\n",
      "iter:  32  finished, D loss:  1.983612 , G loss:  22.089909\n",
      "iter:  33  finished, D loss:  10.3895645 , G loss:  56.610535\n",
      "iter:  34  finished, D loss:  2.8985004 , G loss:  73.66775\n",
      "iter:  35  finished, D loss:  2.4355755 , G loss:  69.70451\n",
      "iter:  36  finished, D loss:  1.0906311 , G loss:  48.32344\n",
      "iter:  37  finished, D loss:  5.0472555 , G loss:  13.191217\n",
      "iter:  38  finished, D loss:  21.1178 , G loss:  48.083454\n",
      "iter:  39  finished, D loss:  2.003665 , G loss:  70.89872\n",
      "iter:  40  finished, D loss:  3.7788014 , G loss:  77.46989\n",
      "iter:  41  finished, D loss:  8.88438 , G loss:  70.834045\n",
      "iter:  42  finished, D loss:  1.6762722 , G loss:  50.93596\n",
      "iter:  43  finished, D loss:  5.4932365 , G loss:  18.09956\n",
      "iter:  44  finished, D loss:  15.85243 , G loss:  58.603867\n",
      "iter:  45  finished, D loss:  5.000596 , G loss:  86.579796\n",
      "iter:  46  finished, D loss:  5.5139403 , G loss:  107.832245\n",
      "iter:  47  finished, D loss:  3.4782004 , G loss:  116.958275\n",
      "iter:  48  finished, D loss:  4.8386602 , G loss:  117.6474\n",
      "iter:  49  finished, D loss:  0.1826402 , G loss:  110.82482\n",
      "iter:  50  finished, D loss:  3.22409 , G loss:  97.80057\n",
      "iter:  51  finished, D loss:  3.383932 , G loss:  80.4809\n",
      "iter:  52  finished, D loss:  3.7048516 , G loss:  57.72047\n",
      "iter:  53  finished, D loss:  1.378964 , G loss:  29.849588\n",
      "iter:  54  finished, D loss:  2.1374185 , G loss:  10.332174\n",
      "iter:  55  finished, D loss:  11.785713 , G loss:  80.7086\n",
      "iter:  56  finished, D loss:  8.094723e-10 , G loss:  135.85016\n",
      "iter:  57  finished, D loss:  2.574315e-09 , G loss:  172.98853\n",
      "iter:  58  finished, D loss:  1.5251036 , G loss:  189.4437\n",
      "iter:  59  finished, D loss:  1.9473262 , G loss:  182.49756\n",
      "iter:  60  finished, D loss:  3.3343031 , G loss:  152.88431\n",
      "iter:  61  finished, D loss:  2.0524585 , G loss:  108.58642\n",
      "iter:  62  finished, D loss:  5.33537 , G loss:  57.04944\n",
      "iter:  63  finished, D loss:  4.2361054 , G loss:  4.5026193\n",
      "iter:  64  finished, D loss:  27.911283 , G loss:  48.780266\n",
      "iter:  65  finished, D loss:  1.5219173 , G loss:  87.44814\n",
      "iter:  66  finished, D loss:  3.5606213 , G loss:  113.382256\n",
      "iter:  67  finished, D loss:  4.877228 , G loss:  128.34164\n",
      "iter:  68  finished, D loss:  1.5250249 , G loss:  134.31735\n",
      "iter:  69  finished, D loss:  4.8313007 , G loss:  132.06313\n",
      "iter:  70  finished, D loss:  1.3704169 , G loss:  121.63567\n",
      "iter:  71  finished, D loss:  0.5716253 , G loss:  105.10959\n",
      "iter:  72  finished, D loss:  2.2664247 , G loss:  80.96538\n",
      "iter:  73  finished, D loss:  0.65393513 , G loss:  52.60498\n",
      "iter:  74  finished, D loss:  0.33282593 , G loss:  19.565619\n",
      "iter:  75  finished, D loss:  4.768919 , G loss:  67.46532\n",
      "iter:  76  finished, D loss:  1.0211214 , G loss:  105.88962\n",
      "iter:  77  finished, D loss:  3.3772316 , G loss:  130.59395\n",
      "iter:  78  finished, D loss:  8.133868 , G loss:  140.59126\n",
      "iter:  79  finished, D loss:  6.5996714 , G loss:  136.56052\n",
      "iter:  80  finished, D loss:  2.9651566 , G loss:  119.81833\n",
      "iter:  81  finished, D loss:  3.5423276 , G loss:  93.98844\n",
      "iter:  82  finished, D loss:  5.1844406 , G loss:  60.73278\n",
      "iter:  83  finished, D loss:  3.2246247e-10 , G loss:  24.85609\n",
      "iter:  84  finished, D loss:  3.0385623 , G loss:  69.37393\n",
      "iter:  85  finished, D loss:  0.75385445 , G loss:  102.45424\n",
      "iter:  86  finished, D loss:  2.2454991 , G loss:  119.16901\n",
      "iter:  87  finished, D loss:  0.92528427 , G loss:  117.93158\n",
      "iter:  88  finished, D loss:  1.0718216 , G loss:  101.39094\n",
      "iter:  89  finished, D loss:  2.7024493 , G loss:  73.918304\n",
      "iter:  90  finished, D loss:  5.265435 , G loss:  41.927055\n",
      "iter:  91  finished, D loss:  1.0224844 , G loss:  14.021711\n",
      "iter:  92  finished, D loss:  10.737336 , G loss:  85.31046\n",
      "iter:  93  finished, D loss:  2.2168581 , G loss:  145.70285\n",
      "iter:  94  finished, D loss:  5.05928 , G loss:  191.04437\n",
      "iter:  95  finished, D loss:  1.6472169 , G loss:  222.39441\n",
      "iter:  96  finished, D loss:  2.7914848 , G loss:  239.27158\n",
      "iter:  97  finished, D loss:  1.5107431 , G loss:  240.04834\n",
      "iter:  98  finished, D loss:  3.40513e-08 , G loss:  231.82544\n",
      "iter:  99  finished, D loss:  3.3884213 , G loss:  211.68796\n",
      "iter:  100  finished, D loss:  3.8012047 , G loss:  185.9252\n",
      "iter:  101  finished, D loss:  6.3814087 , G loss:  156.20325\n",
      "iter:  102  finished, D loss:  4.942086 , G loss:  122.72878\n",
      "iter:  103  finished, D loss:  7.593116 , G loss:  89.39917\n",
      "iter:  104  finished, D loss:  4.1504703 , G loss:  55.826416\n",
      "iter:  105  finished, D loss:  0.018076068 , G loss:  24.454178\n",
      "iter:  106  finished, D loss:  3.8118317 , G loss:  0.7889297\n",
      "iter:  107  finished, D loss:  16.83238 , G loss:  109.458885\n",
      "iter:  108  finished, D loss:  0.7418109 , G loss:  207.14334\n",
      "iter:  109  finished, D loss:  0.8032217 , G loss:  284.74597\n",
      "iter:  110  finished, D loss:  7.482849 , G loss:  337.6797\n",
      "iter:  111  finished, D loss:  1.6438347 , G loss:  366.20804\n",
      "iter:  112  finished, D loss:  11.238673 , G loss:  371.65927\n",
      "iter:  113  finished, D loss:  4.000143 , G loss:  355.1471\n",
      "iter:  114  finished, D loss:  6.6139793 , G loss:  323.0196\n",
      "iter:  115  finished, D loss:  0.6869339 , G loss:  285.29077\n",
      "iter:  116  finished, D loss:  1.7012508 , G loss:  244.1196\n",
      "iter:  117  finished, D loss:  3.4327133 , G loss:  195.3653\n",
      "iter:  118  finished, D loss:  0.5254967 , G loss:  141.4371\n",
      "iter:  119  finished, D loss:  4.077179 , G loss:  86.590576\n",
      "iter:  120  finished, D loss:  3.702731 , G loss:  29.239826\n",
      "iter:  121  finished, D loss:  5.5625906 , G loss:  47.294094\n",
      "iter:  122  finished, D loss:  0.6340291 , G loss:  61.5204\n",
      "iter:  123  finished, D loss:  5.993546 , G loss:  69.4321\n",
      "iter:  124  finished, D loss:  6.269552e-06 , G loss:  65.591095\n",
      "iter:  125  finished, D loss:  5.6822457 , G loss:  53.457542\n",
      "iter:  126  finished, D loss:  0.90402913 , G loss:  39.961197\n",
      "iter:  127  finished, D loss:  2.219857 , G loss:  24.92944\n",
      "iter:  128  finished, D loss:  3.3122506 , G loss:  31.046947\n",
      "iter:  129  finished, D loss:  1.501125 , G loss:  31.438885\n",
      "iter:  130  finished, D loss:  1.9007937 , G loss:  28.264729\n",
      "iter:  131  finished, D loss:  0.9607933 , G loss:  22.464754\n",
      "iter:  132  finished, D loss:  0.20436198 , G loss:  37.52057\n",
      "iter:  133  finished, D loss:  1.37988 , G loss:  49.36078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  134  finished, D loss:  4.5324945 , G loss:  54.79019\n",
      "iter:  135  finished, D loss:  4.213274 , G loss:  52.471542\n",
      "iter:  136  finished, D loss:  2.417805 , G loss:  43.3264\n",
      "iter:  137  finished, D loss:  0.4443249 , G loss:  30.092318\n",
      "iter:  138  finished, D loss:  2.3900902 , G loss:  12.911053\n",
      "iter:  139  finished, D loss:  6.1106634 , G loss:  132.38217\n",
      "iter:  140  finished, D loss:  0.05893588 , G loss:  231.40756\n",
      "iter:  141  finished, D loss:  2.4595644 , G loss:  304.23395\n",
      "iter:  142  finished, D loss:  2.6482277 , G loss:  346.84192\n",
      "iter:  143  finished, D loss:  8.094617 , G loss:  361.91995\n",
      "iter:  144  finished, D loss:  2.4704535 , G loss:  355.13495\n",
      "iter:  145  finished, D loss:  7.572305e-23 , G loss:  336.51694\n",
      "iter:  146  finished, D loss:  4.0449176 , G loss:  311.76266\n",
      "iter:  147  finished, D loss:  5.5752974 , G loss:  282.4547\n",
      "iter:  148  finished, D loss:  2.9323993 , G loss:  250.2993\n",
      "iter:  149  finished, D loss:  2.5699763 , G loss:  216.23099\n",
      "iter:  150  finished, D loss:  2.875627 , G loss:  179.42632\n",
      "iter:  151  finished, D loss:  5.474892 , G loss:  146.55267\n",
      "iter:  152  finished, D loss:  1.1236347e-11 , G loss:  109.26818\n",
      "iter:  153  finished, D loss:  0.5276883 , G loss:  69.58273\n",
      "iter:  154  finished, D loss:  0.8682906 , G loss:  27.772715\n",
      "iter:  155  finished, D loss:  1.276131 , G loss:  29.49282\n",
      "iter:  156  finished, D loss:  3.3292012 , G loss:  59.721394\n",
      "iter:  157  finished, D loss:  1.0575881 , G loss:  82.88805\n",
      "iter:  158  finished, D loss:  0.16694139 , G loss:  95.42208\n",
      "iter:  159  finished, D loss:  7.856987e-05 , G loss:  99.50231\n",
      "iter:  160  finished, D loss:  0.36809313 , G loss:  95.81278\n",
      "iter:  161  finished, D loss:  0.7703664 , G loss:  83.366234\n",
      "iter:  162  finished, D loss:  3.2834845 , G loss:  64.638855\n",
      "iter:  163  finished, D loss:  2.0156689 , G loss:  41.03758\n",
      "iter:  164  finished, D loss:  2.0465105 , G loss:  15.764463\n",
      "iter:  165  finished, D loss:  10.786586 , G loss:  145.62425\n",
      "iter:  166  finished, D loss:  3.1905916 , G loss:  256.51166\n",
      "iter:  167  finished, D loss:  7.4380608 , G loss:  344.31903\n",
      "iter:  168  finished, D loss:  4.2529432e-08 , G loss:  405.80072\n",
      "iter:  169  finished, D loss:  5.14802 , G loss:  440.00082\n",
      "iter:  170  finished, D loss:  3.7990289 , G loss:  449.59357\n",
      "iter:  171  finished, D loss:  5.410529 , G loss:  436.68488\n",
      "iter:  172  finished, D loss:  5.833022 , G loss:  406.23523\n",
      "iter:  173  finished, D loss:  5.088522 , G loss:  358.3824\n",
      "iter:  174  finished, D loss:  5.0549793 , G loss:  299.56387\n",
      "iter:  175  finished, D loss:  4.749009 , G loss:  240.82405\n",
      "iter:  176  finished, D loss:  9.029807 , G loss:  183.34854\n",
      "iter:  177  finished, D loss:  1.0437778 , G loss:  130.54636\n",
      "iter:  178  finished, D loss:  0.82110965 , G loss:  79.092026\n",
      "iter:  179  finished, D loss:  0.6463986 , G loss:  30.0256\n",
      "iter:  180  finished, D loss:  5.95278 , G loss:  0.00018908066\n",
      "iter:  181  finished, D loss:  29.893879 , G loss:  98.6698\n",
      "iter:  182  finished, D loss:  0.5413749 , G loss:  195.7121\n",
      "iter:  183  finished, D loss:  5.88249 , G loss:  274.27173\n",
      "iter:  184  finished, D loss:  16.909906 , G loss:  324.5533\n",
      "iter:  185  finished, D loss:  10.8227 , G loss:  346.03394\n",
      "iter:  186  finished, D loss:  5.8983393 , G loss:  346.71652\n",
      "iter:  187  finished, D loss:  3.428121 , G loss:  329.39783\n",
      "iter:  188  finished, D loss:  5.399264 , G loss:  295.44626\n",
      "iter:  189  finished, D loss:  12.675333 , G loss:  254.43733\n",
      "iter:  190  finished, D loss:  5.090774 , G loss:  209.80762\n",
      "iter:  191  finished, D loss:  6.403011e-23 , G loss:  159.68735\n",
      "iter:  192  finished, D loss:  0.291791 , G loss:  106.60152\n",
      "iter:  193  finished, D loss:  7.765925e-08 , G loss:  53.240646\n",
      "iter:  194  finished, D loss:  7.7532883 , G loss:  3.3109784\n",
      "iter:  195  finished, D loss:  24.258736 , G loss:  101.18036\n",
      "iter:  196  finished, D loss:  1.0410855 , G loss:  185.0892\n",
      "iter:  197  finished, D loss:  2.4521358 , G loss:  251.95343\n",
      "iter:  198  finished, D loss:  5.805411 , G loss:  296.70752\n",
      "iter:  199  finished, D loss:  6.080134 , G loss:  322.66623\n",
      "iter:  200  finished, D loss:  5.0240803 , G loss:  326.51843\n",
      "iter:  201  finished, D loss:  18.461723 , G loss:  312.73383\n",
      "iter:  202  finished, D loss:  17.267889 , G loss:  283.69498\n",
      "iter:  203  finished, D loss:  7.307612 , G loss:  243.80493\n",
      "iter:  204  finished, D loss:  3.5047204 , G loss:  196.32135\n",
      "iter:  205  finished, D loss:  5.2317486 , G loss:  142.92305\n",
      "iter:  206  finished, D loss:  3.3704138 , G loss:  86.077354\n",
      "iter:  207  finished, D loss:  1.4126946 , G loss:  25.758743\n",
      "iter:  208  finished, D loss:  2.594671 , G loss:  81.620605\n",
      "iter:  209  finished, D loss:  1.2407213 , G loss:  128.65561\n",
      "iter:  210  finished, D loss:  9.345654 , G loss:  161.13997\n",
      "iter:  211  finished, D loss:  1.9840918 , G loss:  180.19159\n",
      "iter:  212  finished, D loss:  5.827054 , G loss:  168.4706\n",
      "iter:  213  finished, D loss:  0.1734315 , G loss:  145.41342\n",
      "iter:  214  finished, D loss:  2.055233 , G loss:  107.09501\n",
      "iter:  215  finished, D loss:  3.379974 , G loss:  49.178772\n",
      "iter:  216  finished, D loss:  2.1135118 , G loss:  0.0016228487\n",
      "iter:  217  finished, D loss:  49.867905 , G loss:  162.74821\n",
      "iter:  218  finished, D loss:  4.3220835 , G loss:  305.45\n",
      "iter:  219  finished, D loss:  56.373375 , G loss:  277.81415\n",
      "iter:  220  finished, D loss:  7.386088 , G loss:  204.68861\n",
      "iter:  221  finished, D loss:  4.352297 , G loss:  119.655624\n",
      "iter:  222  finished, D loss:  2.2005463 , G loss:  36.820824\n",
      "iter:  223  finished, D loss:  10.855532 , G loss:  39.509262\n",
      "iter:  224  finished, D loss:  18.9709 , G loss:  101.21605\n",
      "iter:  225  finished, D loss:  4.4270525 , G loss:  148.97484\n",
      "iter:  226  finished, D loss:  11.297688 , G loss:  164.81662\n",
      "iter:  227  finished, D loss:  12.170266 , G loss:  136.0517\n",
      "iter:  228  finished, D loss:  6.8339825 , G loss:  68.01337\n",
      "iter:  229  finished, D loss:  18.677874 , G loss:  3.5337105\n",
      "iter:  230  finished, D loss:  43.61147 , G loss:  189.22357\n",
      "iter:  231  finished, D loss:  57.896576 , G loss:  230.87732\n",
      "iter:  232  finished, D loss:  51.541565 , G loss:  155.32869\n",
      "iter:  233  finished, D loss:  9.378941 , G loss:  46.915215\n",
      "iter:  234  finished, D loss:  1.0925858 , G loss:  1.8185615e-16\n",
      "iter:  235  finished, D loss:  73.028625 , G loss:  80.854385\n",
      "iter:  236  finished, D loss:  4.612093 , G loss:  169.08344\n",
      "iter:  237  finished, D loss:  6.0576706 , G loss:  206.50897\n",
      "iter:  238  finished, D loss:  9.770823 , G loss:  199.04037\n",
      "iter:  239  finished, D loss:  19.09725 , G loss:  141.46362\n",
      "iter:  240  finished, D loss:  8.875048 , G loss:  70.81412\n",
      "iter:  241  finished, D loss:  9.915249 , G loss:  2.2413468\n",
      "iter:  242  finished, D loss:  34.0232 , G loss:  80.426834\n",
      "iter:  243  finished, D loss:  0.59949553 , G loss:  142.21614\n",
      "iter:  244  finished, D loss:  3.0620704 , G loss:  180.1641\n",
      "iter:  245  finished, D loss:  10.79216 , G loss:  181.13521\n",
      "iter:  246  finished, D loss:  10.556476 , G loss:  152.51736\n",
      "iter:  247  finished, D loss:  11.592607 , G loss:  95.288895\n",
      "iter:  248  finished, D loss:  7.613151 , G loss:  32.333084\n",
      "iter:  249  finished, D loss:  8.404898 , G loss:  28.748453\n",
      "iter:  250  finished, D loss:  3.4161844 , G loss:  97.8983\n",
      "iter:  251  finished, D loss:  1.2211805e-06 , G loss:  152.3287\n",
      "iter:  252  finished, D loss:  2.6436863 , G loss:  187.50546\n",
      "iter:  253  finished, D loss:  3.3833747 , G loss:  196.06665\n",
      "iter:  254  finished, D loss:  9.5178995 , G loss:  162.22066\n",
      "iter:  255  finished, D loss:  2.9558241 , G loss:  113.11435\n",
      "iter:  256  finished, D loss:  0.9429511 , G loss:  58.827686\n",
      "iter:  257  finished, D loss:  1.2441217 , G loss:  4.6104946\n",
      "iter:  258  finished, D loss:  22.145813 , G loss:  113.26451\n",
      "iter:  259  finished, D loss:  1.658982 , G loss:  205.05016\n",
      "iter:  260  finished, D loss:  14.423147 , G loss:  248.86539\n",
      "iter:  261  finished, D loss:  27.603569 , G loss:  234.5253\n",
      "iter:  262  finished, D loss:  11.388893 , G loss:  190.1801\n",
      "iter:  263  finished, D loss:  9.73334 , G loss:  123.8377\n",
      "iter:  264  finished, D loss:  5.3343277 , G loss:  38.456604\n",
      "iter:  265  finished, D loss:  7.033905 , G loss:  50.26497\n",
      "iter:  266  finished, D loss:  5.88806 , G loss:  59.888756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  267  finished, D loss:  9.092229 , G loss:  51.736145\n",
      "iter:  268  finished, D loss:  11.348932 , G loss:  52.20452\n",
      "iter:  269  finished, D loss:  12.209242 , G loss:  72.00994\n",
      "iter:  270  finished, D loss:  10.238792 , G loss:  72.39055\n",
      "iter:  271  finished, D loss:  11.89095 , G loss:  25.339058\n",
      "iter:  272  finished, D loss:  14.870508 , G loss:  164.03496\n",
      "iter:  273  finished, D loss:  24.963936 , G loss:  220.8203\n",
      "iter:  274  finished, D loss:  52.293568 , G loss:  188.10925\n",
      "iter:  275  finished, D loss:  10.120672 , G loss:  136.01337\n",
      "iter:  276  finished, D loss:  0.06795784 , G loss:  80.20645\n",
      "iter:  277  finished, D loss:  2.8027873e-11 , G loss:  19.46193\n",
      "iter:  278  finished, D loss:  14.136407 , G loss:  99.55826\n",
      "iter:  279  finished, D loss:  0.35139287 , G loss:  155.00545\n",
      "iter:  280  finished, D loss:  9.9731045 , G loss:  187.39243\n",
      "iter:  281  finished, D loss:  6.043475 , G loss:  194.24577\n",
      "iter:  282  finished, D loss:  10.018383 , G loss:  178.54407\n",
      "iter:  283  finished, D loss:  20.188248 , G loss:  129.31483\n",
      "iter:  284  finished, D loss:  5.5718765 , G loss:  69.3714\n",
      "iter:  285  finished, D loss:  4.9791827 , G loss:  11.399535\n",
      "iter:  286  finished, D loss:  25.299564 , G loss:  92.26086\n",
      "iter:  287  finished, D loss:  9.753469 , G loss:  151.47638\n",
      "iter:  288  finished, D loss:  11.888474 , G loss:  178.7724\n",
      "iter:  289  finished, D loss:  14.145164 , G loss:  164.02596\n",
      "iter:  290  finished, D loss:  7.206982 , G loss:  127.58752\n",
      "iter:  291  finished, D loss:  4.7784233 , G loss:  76.90839\n",
      "iter:  292  finished, D loss:  1.9935308 , G loss:  26.574142\n",
      "iter:  293  finished, D loss:  12.390189 , G loss:  72.50252\n",
      "iter:  294  finished, D loss:  5.4348717 , G loss:  101.49141\n",
      "iter:  295  finished, D loss:  6.6018147 , G loss:  116.25357\n",
      "iter:  296  finished, D loss:  6.02003 , G loss:  110.736404\n",
      "iter:  297  finished, D loss:  1.5766102 , G loss:  91.99304\n",
      "iter:  298  finished, D loss:  0.38875073 , G loss:  63.996895\n",
      "iter:  299  finished, D loss:  3.7798047 , G loss:  27.822052\n",
      "iter:  300  finished, D loss:  9.163729 , G loss:  68.664474\n",
      "iter:  301  finished, D loss:  4.666129 , G loss:  86.885544\n",
      "iter:  302  finished, D loss:  6.901764 , G loss:  75.25019\n",
      "iter:  303  finished, D loss:  5.0374775 , G loss:  37.934845\n",
      "iter:  304  finished, D loss:  6.2805147 , G loss:  30.955395\n",
      "iter:  305  finished, D loss:  6.935117 , G loss:  111.95163\n",
      "iter:  306  finished, D loss:  9.423285 , G loss:  155.03426\n",
      "iter:  307  finished, D loss:  9.541518 , G loss:  157.67764\n",
      "iter:  308  finished, D loss:  5.5613146 , G loss:  143.51634\n",
      "iter:  309  finished, D loss:  0.9597091 , G loss:  115.753586\n",
      "iter:  310  finished, D loss:  9.338534 , G loss:  74.1886\n",
      "iter:  311  finished, D loss:  2.4760308 , G loss:  39.469433\n",
      "iter:  312  finished, D loss:  2.7851615 , G loss:  61.855473\n",
      "iter:  313  finished, D loss:  4.9014173 , G loss:  71.547554\n",
      "iter:  314  finished, D loss:  2.3161008 , G loss:  66.67927\n",
      "iter:  315  finished, D loss:  0.00032961922 , G loss:  56.186375\n",
      "iter:  316  finished, D loss:  0.011980166 , G loss:  37.79905\n",
      "iter:  317  finished, D loss:  1.5960066 , G loss:  61.055656\n",
      "iter:  318  finished, D loss:  0.1834937 , G loss:  67.78361\n",
      "iter:  319  finished, D loss:  6.5777473 , G loss:  43.921448\n",
      "iter:  320  finished, D loss:  5.8226213 , G loss:  47.570656\n",
      "iter:  321  finished, D loss:  8.064676 , G loss:  64.58373\n",
      "iter:  322  finished, D loss:  3.9324694 , G loss:  82.38909\n",
      "iter:  323  finished, D loss:  13.233995 , G loss:  85.913445\n",
      "iter:  324  finished, D loss:  4.395954 , G loss:  76.64042\n",
      "iter:  325  finished, D loss:  3.6700416 , G loss:  58.498856\n",
      "iter:  326  finished, D loss:  3.7448149 , G loss:  38.80745\n",
      "iter:  327  finished, D loss:  15.063504 , G loss:  49.014854\n",
      "iter:  328  finished, D loss:  12.36197 , G loss:  56.83696\n",
      "iter:  329  finished, D loss:  15.221095 , G loss:  55.20541\n",
      "iter:  330  finished, D loss:  1.2034909 , G loss:  48.10647\n",
      "iter:  331  finished, D loss:  2.3944798 , G loss:  42.92314\n",
      "iter:  332  finished, D loss:  2.0598538 , G loss:  63.19176\n",
      "iter:  333  finished, D loss:  7.6647153 , G loss:  69.06473\n",
      "iter:  334  finished, D loss:  3.6306808 , G loss:  60.06001\n",
      "iter:  335  finished, D loss:  3.957436 , G loss:  46.41512\n",
      "iter:  336  finished, D loss:  1.9945931 , G loss:  46.62169\n",
      "iter:  337  finished, D loss:  11.568505 , G loss:  52.219578\n",
      "iter:  338  finished, D loss:  9.263148 , G loss:  56.396973\n",
      "iter:  339  finished, D loss:  7.055109 , G loss:  50.92625\n",
      "iter:  340  finished, D loss:  3.9679835 , G loss:  40.48856\n",
      "iter:  341  finished, D loss:  6.4299827 , G loss:  35.65271\n",
      "iter:  342  finished, D loss:  7.109552 , G loss:  38.828716\n",
      "iter:  343  finished, D loss:  2.5078871 , G loss:  42.49507\n",
      "iter:  344  finished, D loss:  3.6074433 , G loss:  37.060387\n",
      "iter:  345  finished, D loss:  8.427606 , G loss:  47.099777\n",
      "iter:  346  finished, D loss:  1.5434333 , G loss:  47.173798\n",
      "iter:  347  finished, D loss:  1.8766237 , G loss:  33.871887\n",
      "iter:  348  finished, D loss:  4.27382 , G loss:  65.12982\n",
      "iter:  349  finished, D loss:  3.9231567 , G loss:  76.58469\n",
      "iter:  350  finished, D loss:  10.660407 , G loss:  71.77983\n",
      "iter:  351  finished, D loss:  4.325041 , G loss:  44.840782\n",
      "iter:  352  finished, D loss:  5.089372 , G loss:  62.59784\n",
      "iter:  353  finished, D loss:  13.445591 , G loss:  72.66012\n",
      "iter:  354  finished, D loss:  2.5789826 , G loss:  69.28769\n",
      "iter:  355  finished, D loss:  23.179146 , G loss:  40.629883\n",
      "iter:  356  finished, D loss:  12.748145 , G loss:  81.36617\n",
      "iter:  357  finished, D loss:  3.5190856 , G loss:  98.39342\n",
      "iter:  358  finished, D loss:  15.265674 , G loss:  86.13762\n",
      "iter:  359  finished, D loss:  4.3822145 , G loss:  52.926437\n",
      "iter:  360  finished, D loss:  2.6854467 , G loss:  39.125862\n",
      "iter:  361  finished, D loss:  9.141391 , G loss:  136.53754\n",
      "iter:  362  finished, D loss:  7.2005186 , G loss:  204.58133\n",
      "iter:  363  finished, D loss:  21.623163 , G loss:  232.6641\n",
      "iter:  364  finished, D loss:  15.143354 , G loss:  218.59006\n",
      "iter:  365  finished, D loss:  21.389732 , G loss:  163.45126\n",
      "iter:  366  finished, D loss:  15.0256195 , G loss:  84.99287\n",
      "iter:  367  finished, D loss:  5.7659154 , G loss:  5.3386803\n",
      "iter:  368  finished, D loss:  42.59905 , G loss:  101.71557\n",
      "iter:  369  finished, D loss:  18.034943 , G loss:  164.69925\n",
      "iter:  370  finished, D loss:  7.122583 , G loss:  190.9627\n",
      "iter:  371  finished, D loss:  14.648924 , G loss:  182.36507\n",
      "iter:  372  finished, D loss:  14.471659 , G loss:  137.58282\n",
      "iter:  373  finished, D loss:  16.244486 , G loss:  66.352356\n",
      "iter:  374  finished, D loss:  12.5102005 , G loss:  16.773912\n",
      "iter:  375  finished, D loss:  36.49457 , G loss:  128.30005\n",
      "iter:  376  finished, D loss:  2.9510832 , G loss:  221.12816\n",
      "iter:  377  finished, D loss:  7.72703 , G loss:  275.627\n",
      "iter:  378  finished, D loss:  12.296904 , G loss:  280.4766\n",
      "iter:  379  finished, D loss:  8.3677435 , G loss:  251.91678\n",
      "iter:  380  finished, D loss:  13.37583 , G loss:  195.38075\n",
      "iter:  381  finished, D loss:  8.599545 , G loss:  107.09796\n",
      "iter:  382  finished, D loss:  8.962334 , G loss:  7.598954\n",
      "iter:  383  finished, D loss:  44.896336 , G loss:  114.17911\n",
      "iter:  384  finished, D loss:  15.316467 , G loss:  168.04263\n",
      "iter:  385  finished, D loss:  11.960392 , G loss:  155.34259\n",
      "iter:  386  finished, D loss:  23.593342 , G loss:  97.09279\n",
      "iter:  387  finished, D loss:  17.27642 , G loss:  5.2956314\n",
      "iter:  388  finished, D loss:  54.80489 , G loss:  113.1123\n",
      "iter:  389  finished, D loss:  19.63619 , G loss:  167.52954\n",
      "iter:  390  finished, D loss:  22.425537 , G loss:  184.9559\n",
      "iter:  391  finished, D loss:  17.491573 , G loss:  162.38095\n",
      "iter:  392  finished, D loss:  9.186039 , G loss:  106.956474\n",
      "iter:  393  finished, D loss:  14.326488 , G loss:  42.12095\n",
      "iter:  394  finished, D loss:  12.770283 , G loss:  51.998672\n",
      "iter:  395  finished, D loss:  5.143399 , G loss:  77.43474\n",
      "iter:  396  finished, D loss:  11.189374 , G loss:  84.5428\n",
      "iter:  397  finished, D loss:  10.570871 , G loss:  76.48431\n",
      "iter:  398  finished, D loss:  25.10193 , G loss:  26.766438\n",
      "iter:  399  finished, D loss:  23.88404 , G loss:  118.41225\n",
      "iter:  400  finished, D loss:  18.865326 , G loss:  174.37619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  401  finished, D loss:  22.965118 , G loss:  173.10349\n",
      "iter:  402  finished, D loss:  18.029402 , G loss:  142.54984\n",
      "iter:  403  finished, D loss:  22.369003 , G loss:  78.7437\n",
      "iter:  404  finished, D loss:  8.610967 , G loss:  2.1035047\n",
      "iter:  405  finished, D loss:  37.385254 , G loss:  159.9603\n",
      "iter:  406  finished, D loss:  27.424751 , G loss:  250.9507\n",
      "iter:  407  finished, D loss:  22.290596 , G loss:  276.44708\n",
      "iter:  408  finished, D loss:  38.21186 , G loss:  208.22745\n",
      "iter:  409  finished, D loss:  10.853788 , G loss:  109.147964\n",
      "iter:  410  finished, D loss:  4.7146606 , G loss:  1.8690104\n",
      "iter:  411  finished, D loss:  48.420284 , G loss:  138.99693\n",
      "iter:  412  finished, D loss:  8.12963 , G loss:  248.14415\n",
      "iter:  413  finished, D loss:  19.741566 , G loss:  266.6512\n",
      "iter:  414  finished, D loss:  28.748327 , G loss:  195.29681\n",
      "iter:  415  finished, D loss:  13.160086 , G loss:  87.99536\n",
      "iter:  416  finished, D loss:  0.7547509 , G loss:  0.31801367\n",
      "iter:  417  finished, D loss:  80.18139 , G loss:  134.20053\n",
      "iter:  418  finished, D loss:  10.537422 , G loss:  237.26053\n",
      "iter:  419  finished, D loss:  22.864407 , G loss:  257.02444\n",
      "iter:  420  finished, D loss:  68.308685 , G loss:  172.9254\n",
      "iter:  421  finished, D loss:  11.279045 , G loss:  45.167587\n",
      "iter:  422  finished, D loss:  14.725674 , G loss:  22.95726\n",
      "iter:  423  finished, D loss:  21.86781 , G loss:  189.026\n",
      "iter:  424  finished, D loss:  17.66226 , G loss:  266.82764\n",
      "iter:  425  finished, D loss:  37.934418 , G loss:  240.31651\n",
      "iter:  426  finished, D loss:  13.244284 , G loss:  161.40579\n",
      "iter:  427  finished, D loss:  2.9675927 , G loss:  45.630974\n",
      "iter:  428  finished, D loss:  15.1702 , G loss:  45.704865\n",
      "iter:  429  finished, D loss:  14.663977 , G loss:  159.56207\n",
      "iter:  430  finished, D loss:  4.0262227 , G loss:  217.20346\n",
      "iter:  431  finished, D loss:  52.671864 , G loss:  149.86467\n",
      "iter:  432  finished, D loss:  23.29116 , G loss:  63.2426\n",
      "iter:  433  finished, D loss:  3.9563453 , G loss:  9.001755\n",
      "iter:  434  finished, D loss:  30.989315 , G loss:  125.42139\n",
      "iter:  435  finished, D loss:  1.4564642 , G loss:  217.37604\n",
      "iter:  436  finished, D loss:  0.20859632 , G loss:  285.30994\n",
      "iter:  437  finished, D loss:  17.451649 , G loss:  315.56577\n",
      "iter:  438  finished, D loss:  8.605222 , G loss:  330.73877\n",
      "iter:  439  finished, D loss:  5.89933 , G loss:  320.92896\n",
      "iter:  440  finished, D loss:  3.8211927 , G loss:  274.53613\n",
      "iter:  441  finished, D loss:  6.217611 , G loss:  227.09708\n",
      "iter:  442  finished, D loss:  4.3191672e-09 , G loss:  168.45663\n",
      "iter:  443  finished, D loss:  6.6363206 , G loss:  82.71724\n",
      "iter:  444  finished, D loss:  1.991165 , G loss:  12.331233\n",
      "iter:  445  finished, D loss:  49.225925 , G loss:  57.317757\n",
      "iter:  446  finished, D loss:  18.023594 , G loss:  150.93854\n",
      "iter:  447  finished, D loss:  1.2000804 , G loss:  219.23387\n",
      "iter:  448  finished, D loss:  10.621099 , G loss:  254.0961\n",
      "iter:  449  finished, D loss:  8.277181 , G loss:  262.52844\n",
      "iter:  450  finished, D loss:  17.018837 , G loss:  254.86743\n",
      "iter:  451  finished, D loss:  20.795479 , G loss:  230.4306\n",
      "iter:  452  finished, D loss:  11.197274 , G loss:  198.36206\n",
      "iter:  453  finished, D loss:  6.553894 , G loss:  166.4331\n",
      "iter:  454  finished, D loss:  5.650445 , G loss:  127.390656\n",
      "iter:  455  finished, D loss:  4.6447182e-30 , G loss:  91.13153\n",
      "iter:  456  finished, D loss:  0.5660659 , G loss:  55.0923\n",
      "iter:  457  finished, D loss:  4.5529203 , G loss:  19.066551\n",
      "iter:  458  finished, D loss:  14.437679 , G loss:  68.12684\n",
      "iter:  459  finished, D loss:  1.18169785e-08 , G loss:  108.36371\n",
      "iter:  460  finished, D loss:  2.4631963 , G loss:  129.85541\n",
      "iter:  461  finished, D loss:  6.2768135 , G loss:  126.85248\n",
      "iter:  462  finished, D loss:  4.542441 , G loss:  107.350494\n",
      "iter:  463  finished, D loss:  5.4842787 , G loss:  58.851246\n",
      "iter:  464  finished, D loss:  4.7312727 , G loss:  44.078762\n",
      "iter:  465  finished, D loss:  14.84018 , G loss:  124.63191\n",
      "iter:  466  finished, D loss:  1.6417506e-11 , G loss:  178.08478\n",
      "iter:  467  finished, D loss:  3.066066 , G loss:  198.44801\n",
      "iter:  468  finished, D loss:  3.1114614 , G loss:  193.1996\n",
      "iter:  469  finished, D loss:  5.168615 , G loss:  170.03714\n",
      "iter:  470  finished, D loss:  8.018041 , G loss:  131.07556\n",
      "iter:  471  finished, D loss:  3.9881809 , G loss:  67.615685\n",
      "iter:  472  finished, D loss:  17.051851 , G loss:  49.21321\n",
      "iter:  473  finished, D loss:  19.732971 , G loss:  132.47171\n",
      "iter:  474  finished, D loss:  20.037357 , G loss:  173.64586\n",
      "iter:  475  finished, D loss:  7.79749 , G loss:  180.64127\n",
      "iter:  476  finished, D loss:  0.086017214 , G loss:  154.21265\n",
      "iter:  477  finished, D loss:  4.3776493 , G loss:  107.91359\n",
      "iter:  478  finished, D loss:  7.9494095 , G loss:  48.143166\n",
      "iter:  479  finished, D loss:  29.426918 , G loss:  80.09186\n",
      "iter:  480  finished, D loss:  6.5358696 , G loss:  113.67914\n",
      "iter:  481  finished, D loss:  12.802838 , G loss:  121.05658\n",
      "iter:  482  finished, D loss:  22.876808 , G loss:  106.037994\n",
      "iter:  483  finished, D loss:  10.560938 , G loss:  66.53072\n",
      "iter:  484  finished, D loss:  13.099339 , G loss:  47.71099\n",
      "iter:  485  finished, D loss:  23.07759 , G loss:  79.44897\n",
      "iter:  486  finished, D loss:  4.541253 , G loss:  105.94092\n",
      "iter:  487  finished, D loss:  4.220589 , G loss:  111.8616\n",
      "iter:  488  finished, D loss:  7.8336325 , G loss:  98.20857\n",
      "iter:  489  finished, D loss:  7.9888897 , G loss:  67.472244\n",
      "iter:  490  finished, D loss:  3.7971916 , G loss:  43.35882\n",
      "iter:  491  finished, D loss:  24.156729 , G loss:  86.49348\n",
      "iter:  492  finished, D loss:  8.79047 , G loss:  106.310936\n",
      "iter:  493  finished, D loss:  9.405752 , G loss:  107.23537\n",
      "iter:  494  finished, D loss:  2.934553 , G loss:  100.493385\n",
      "iter:  495  finished, D loss:  6.4840226 , G loss:  87.753525\n",
      "iter:  496  finished, D loss:  3.1006866 , G loss:  64.87529\n",
      "iter:  497  finished, D loss:  6.4769797 , G loss:  44.95443\n",
      "iter:  498  finished, D loss:  13.1144085 , G loss:  79.065956\n",
      "iter:  499  finished, D loss:  8.289437 , G loss:  102.38221\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# Model training process.\n",
    "# After training, model will be stored under model root\n",
    "# We have trained our model 200,000 iterations on our computers, and stored as 'graph.pb'.\n",
    "# Here, we only trained 500 iterations to show our loss.\n",
    "\n",
    "dataset_path = '/home/ecbm4040/data'\n",
    "model_root = '/home/ecbm4040/models'\n",
    "g_learning_rate = 0.0002\n",
    "d_learning_rate = 0.0002\n",
    "batch_size = 64\n",
    "#iteration = 10000\n",
    "iteration = 500\n",
    "dim = 100\n",
    "image_size = 64\n",
    "\n",
    "train.train(dataset_path, batch_size, model_root, d_learning_rate, g_learning_rate, iteration, image_size,  dim, istrain = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generator loss and discriminator loss from our model that trained 200,000 iterations\n",
    "![loss graph](./log/my_lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
